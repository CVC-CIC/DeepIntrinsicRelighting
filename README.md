# Relighting from a Single Image: Datasets and Deep Intrinsic-based Architecture
Yixiong Yang, Hassan Ahmed Sial, Ramon Baldrich, Maria Vanrell

[Arxiv](https://arxiv.org/abs/2409.18770)

## Description
This repo contains the official code, data, and video results for the paper. 


https://github.com/liulisixin/save_videos/assets/49985369/c375b269-032e-4e2c-9685-18fa4f5e7a11

## Setup
The code was tested on PyTorch 1.10.1, but it is not strict; other versions should also work.
```
conda create --name DIR python=3.7
conda activate DIR
pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html
pip install -r requirements.txt
```


## Datasets
### ISR: Intrinsic Scene Relighting Dataset
[Reflectance](https://cvcuab-my.sharepoint.com/:u:/g/personal/yixiong_cvc_uab_cat/Ed4PMW9cxJBKipa2GNlSvSQBD7try__Gz6Sk76Qbwcx7nA?e=lHvddG)
[Shading](https://cvcuab-my.sharepoint.com/:u:/g/personal/yixiong_cvc_uab_cat/EdBYF2GUO35Hpsm_PBpEMsQBjstij4hOOn2HlxQ4ekDwqw?e=aQukUd)
[Image](https://cvcuab-my.sharepoint.com/:u:/g/personal/yixiong_cvc_uab_cat/EfeCiWYw_P9BnVqub6ii8FYBjtRVyMwko3E-av8WZTTo1Q?e=EQdM6E)

You can download only the Reflectance and Shading, as the image can be calculated from them in the code.

### RSR: Real Scene Relighting Dataset
[Download](https://cvcuab-my.sharepoint.com/:u:/g/personal/yixiong_cvc_uab_cat/ETWcj5yBKgJLqUZDsT9Q39QBJ8GUJYEQuzNWpV5FS2lPRg?e=jEYI5t)

In the code, the path of datasets can be modified in the self.server_root of options/base_options.py. 
## Train
Train from the scratch on the ISR dataset:
```angular2html
CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 --master_port 7777 train.py isr   # For ISR dataset
```
Continue training on other datasets (place the pre-trained model in checkpoints/{exp} with the name base_{}):
```angular2html
CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 --master_port 7777 train.py rsr_ours_f   # For RSR dataset
CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 --master_port 7777 train.py vidit_ours_f   # For VIDIT dataset
CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 --master_port 7777 train.py multilum_ours_f   # For Multi-illumination dataset
```
**Note**: 
1. When using the VIDIT dataset, place all images into a single folder to create a complete version, named VIDIT_full.
2. When using the Multi-illumination dataset, crop and resize the images to a resolution of 256x256. The code for this process can be found in ./data/multi_illumination/crop_resize.py.


## Test
You can download the checkpoints from this link [Checkpoints](https://cvcuab-my.sharepoint.com/:u:/g/personal/yixiong_cvc_uab_cat/EZGyWdlf5nhIpzMeCDpp2DwBsnLfmExrl9NeO_KY8w60Ow?e=540jgE), and do the tests quantitatively or qualitatively. 
```angular2html
python test_quantitative.py --name {$exp}
```

```angular2html
python test_qualitative.py --name {$exp}
```
The name can be 'exp_isr', 'exp_rsr_ours_f', 'exp_vidit_ours_f' and 'exp_multilum_ours_f'. 

The video results can be generated by:
```angular2html
python test_qualitative_animation.py --name exp_isr
```


## More video results

https://github.com/liulisixin/save_videos/assets/49985369/6dc22788-87ee-4836-8677-7d4f4d8c3b7c

https://github.com/liulisixin/save_videos/assets/49985369/57a1efd7-7f8b-460c-84a0-9f3dd596da56


# Citation
Please cite this repository as follows if you find it helpful for your project :):
```
@article{yang2024relighting,
  title={Relighting from a Single Image: Datasets and Deep Intrinsic-based Architecture},
  author={Yang, Yixiong and Sial, Hassan Ahmed and Baldrich, Ramon and Vanrell, Maria},
  journal={arXiv preprint arXiv:2409.18770},
  year={2024}
}
```

## Acknowledgments
Some codes are borrowed from [pix2pix](https://github.com/phillipi/pix2pix). Thanks for their great works.

